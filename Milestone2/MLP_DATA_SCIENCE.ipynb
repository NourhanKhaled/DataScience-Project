{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import random\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data_train = pd.read_csv('production_data_train.csv')\n",
    "prod_data_test = pd.read_csv('production_data_test.csv')\n",
    "ihs_data = pd.read_csv('IHS_data.csv')\n",
    "harmony_data = pd.read_csv('Harmony_data.csv')\n",
    "test_apis = pd.read_csv('test_APIs.csv', header=None)\n",
    "test_apis.columns = ['API']\n",
    "test_apis['API'] = (test_apis['API']).astype(str)\n",
    "test_apis['API'] = test_apis['API'].apply(lambda x: x.zfill(14))\n",
    "sample_file = pd.read_csv('sample_file.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(prod_data, wells_data, train):\n",
    "    prod_data = prod_data.drop_duplicates(subset=['API', 'Month', 'Year'], keep='last', inplace=False)\n",
    "    \n",
    "    # adding zeros to API\n",
    "    prod_data['API'] = (prod_data['API']).astype(str)\n",
    "    prod_data['API'] = prod_data['API'].apply(lambda x: x.zfill(14))\n",
    "    \n",
    "    # adding zeros to API\n",
    "    wells_data['API'] = (wells_data['API']).astype(str)\n",
    "    wells_data['API'] = wells_data['API'].apply(lambda x: x.zfill(14))\n",
    "    \n",
    "    # storing peak records \n",
    "    idx_max = prod_data.groupby(['API'])['Liquid'].transform('max') == prod_data['Liquid']\n",
    "    max_month_prod_data = prod_data[idx_max].drop_duplicates(subset='API', keep='first', inplace=False)\n",
    "    list_indices = ['API', 'Year', 'Month', 'Liquid']\n",
    "    max_month_prod_data = max_month_prod_data[list_indices]\n",
    "    max_month_prod_data = max_month_prod_data.rename(columns={\"Year\": \"Max_Year\", \"Month\": \"Max_Month\", \"Liquid\": \"Max_Liquid\"})\n",
    "    \n",
    "    # merging the two dataframes to get max month and max year\n",
    "    new_prod_data_orig = prod_data.merge(max_month_prod_data, on='API')\n",
    "#     print(new_prod_data_orig['API'].drop_duplicates())\n",
    "    # Remove Pre-Peak Months (clean up)\n",
    "    new_prod_data = new_prod_data_orig[((new_prod_data_orig['Year'] == new_prod_data_orig['Max_Year']))]\n",
    "    new_prod_data = new_prod_data[(new_prod_data['Month'] >= new_prod_data['Max_Month'])]\n",
    "\n",
    "    new_prod_data2 = new_prod_data_orig[((new_prod_data_orig['Year'] > new_prod_data_orig['Max_Year']))]\n",
    "    new_prod_data3 = new_prod_data.append(new_prod_data2)\n",
    "    # adding month index column to post peak production data\n",
    "    new_prod_data3['index'] = calc_month_index(new_prod_data3['Max_Year'], new_prod_data3['Max_Month'], new_prod_data3['Year'], new_prod_data3['Month'])\n",
    "    indexed_prod_data = new_prod_data3\n",
    "    \n",
    "    # removed nullified SpudDates and CompletionDates\n",
    "    wells_data = wells_data[~((wells_data['SpudDate'].isnull()) & (wells_data['CompletionDate'].isnull()))]\n",
    "    \n",
    "    # replacing null CompletionDates with SpudDates + six months\n",
    "    wells_data['SpudDate'] = pd.to_datetime(wells_data['SpudDate'])\n",
    "    wells_data['CompletionDate'] = pd.to_datetime(wells_data['CompletionDate'])\n",
    "    wells_data.loc[wells_data['CompletionDate'].isnull(), 'CompletionDate'] = wells_data['SpudDate'] + timedelta(days=170) \n",
    "    \n",
    "    # replacing StateNames with indices\n",
    "    unique_state_names = wells_data.StateName.unique()\n",
    "    unique_state_ids = list(range(0, len(unique_state_names)))\n",
    "    dict_state_names = dict(zip( unique_state_names, unique_state_ids))\n",
    "    wells_data['StateName'] = wells_data['StateName'].map(dict_state_names)\n",
    "                            \n",
    "    # replacing CountyNames with indices                         \n",
    "    unique_county_names = wells_data.CountyName.unique()\n",
    "    unique_county_ids = list(range(0, len(unique_county_names)))\n",
    "    dict_county_names = dict(zip(unique_county_names, unique_county_ids))\n",
    "    wells_data['CountyName'] = wells_data['CountyName'].map(dict_county_names)\n",
    "    \n",
    "    # replacing BasinName with indices\n",
    "    unique_basin_names = wells_data.BasinName.unique()\n",
    "    unique_basin_ids = list(range(0, len(unique_basin_names)))\n",
    "    dict_basin_names = dict(zip(unique_basin_names, unique_basin_ids))\n",
    "    wells_data['BasinName'] = wells_data['BasinName'].map(dict_basin_names)\n",
    "    \n",
    "    #replacing Formation with indices\n",
    "    unique_formation_names = wells_data.formation.unique()\n",
    "    unique_formation_ids = list(range(0, len(unique_formation_names)))\n",
    "    dict_formation_names = dict(zip(unique_formation_names, unique_formation_ids))\n",
    "    wells_data['formation'] = wells_data['formation'].map(dict_formation_names)\n",
    "        \n",
    "#     wells_data[wells_data['CompletionDate'] >= pd.Timestamp(2014, 1 , 1)]['CompletionDate'] = 1  \n",
    "#     wells_data[wells_data['CompletionDate'] !=  1]['CompletionDate'] = 0 \n",
    "#     print(wells_data[wells_data['CompletionDate'] < pd.Timestamp(2014, 1 , 1)])\n",
    "\n",
    "    indexed_prod_data = indexed_prod_data.merge(wells_data, on='API')\n",
    "    three_years_data = indexed_prod_data\n",
    "    if(train):\n",
    "        three_years_data = indexed_prod_data[(indexed_prod_data['Max_Year'] < 2016) |((indexed_prod_data['Max_Year'] == 2016) & (indexed_prod_data['Max_Month'] == 1))] \n",
    "        three_years_data = three_years_data[three_years_data['index'] <= 36] \n",
    "    \n",
    "    return three_years_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates month index\n",
    "def calc_month_index(max_year, max_month, year, month):\n",
    "    return (12 - max_month + (year - max_year - 1)*12 + month) * (year != max_year) + (year == max_year) * (month - max_month) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Dell\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "processed_train = preprocess(prod_data_train, ihs_data, True)\n",
    "processed_test = preprocess(prod_data_test, ihs_data, False)\n",
    "\n",
    "harmony_data['API'] = (harmony_data['API']).astype(str)\n",
    "harmony_data['API'] = harmony_data['API'].apply(lambda x: x.zfill(14))\n",
    "\n",
    "harmony_data[harmony_data['WATER_PER_FOOT'].isnull()] = 0\n",
    "harmony_data[harmony_data['PROP_PER_FOOT'].isnull()] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "complete_prod_train = processed_train.merge(harmony_data, on='API')\n",
    "complete_prod_test = processed_test.merge(harmony_data, on='API', how='outer')\n",
    "complete_prod_test.fillna(0, inplace=True)\n",
    "complete_prod_test = complete_prod_test.merge(test_apis, on='API')\n",
    "# removing production of month index greater that 3\n",
    "# complete_prod_test = complete_prod_test[complete_prod_test['index'] <= 3]\n",
    "\n",
    "# replacing gor nans with zeros\n",
    "complete_prod_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_train = ['Gas', 'Water', 'Max_Year','operatorNameIHS', 'CompletionDate', 'FirstProductionDate', 'Max_Liquid', \n",
    "              'Max_Month', 'SpudDate', 'PermitDate', '_LastUpdate','BasinName', 'StateName', 'CountyName', 'DaysOn']\n",
    "cols_test = ['Gas', 'date', 'Water', 'Max_Year','operatorNameIHS', 'CompletionDate', 'FirstProductionDate', 'Max_Liquid',\n",
    "             'Max_Month', 'SpudDate', 'PermitDate', '_LastUpdate', 'BasinName', 'StateName','CountyName', 'DaysOn']\n",
    "complete_prod_train = complete_prod_train.drop(cols_train, axis=1)\n",
    "complete_prod_test = complete_prod_test.drop(cols_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3331"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_prod_test['API'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input first three months and y label is cumulative\n",
    "# predict missing entries in test data by averaging\n",
    "# in train disregard qthings with less than 36\n",
    "testing_apis = []\n",
    "def get_three_months(prod_data, feat_arr, y_label, test):\n",
    "    prod_data = (prod_data.groupby('API').apply(generate_input, feat_arr, y_label, test))\n",
    "    \n",
    "def generate_input(group, feat_arr, y_label, test):\n",
    "    global testing_apis\n",
    "    y_label.append(group['Liquid'].sum())\n",
    "    testing_apis.append(group['API'].max())\n",
    "    if len(group) < 2:\n",
    "        print(\"API\", group['API'])\n",
    "        \n",
    "    group = group.drop(['API'], axis=1)\n",
    "    records = group[:3]\n",
    "        \n",
    "    if len(records) == 3 and not test:\n",
    "        feat_arr.append(list(np.array(records).flatten()))\n",
    "        \n",
    "    elif test and len(records) < 3:\n",
    "        idx_1 = records[records['index'] == 1]\n",
    "        idx_2 = records[records['index'] == 2]\n",
    "        idx_3 = records[records['index'] == 3]\n",
    "                \n",
    "#         if idx_1['Liquid'].isnan():\n",
    "#             # case 1, idx 1 missing\n",
    "#             missing_rec = idx_2\n",
    "#             missing_rec['Liquid'] = idx_2['Liquid'] + (idx_2['Liquid'] - idx_3['Liquid']).abs()\n",
    "#             missing_rec['index'] = 1\n",
    "#             arr = np.vstack((missing_rec, idx_2, idx_3))\n",
    "#             feat_arr.append(list(arr.flatten()))\n",
    "        \n",
    "#         if idx_2['Liquid'].isnan():\n",
    "#             # case 2, idx 2 missing\n",
    "        missing_rec = idx_1\n",
    "        missing_rec['Liquid'] = 4 #(idx_1['Liquid'] + idx_3['Liquid'])/2\n",
    "        missing_rec['index'] = 2\n",
    "        arr = np.vstack((idx_1, missing_rec, idx_3))\n",
    "        feat_arr.append(list(arr.flatten()))\n",
    "\n",
    "#         if idx_3['Liquid'].isnan():\n",
    "#             # case 3, idx 3 missing\n",
    "#             missing_rec = idx_1\n",
    "#             missing_rec['Liquid'] = (idx_1['Liquid'] - idx_2['Liquid']).abs() + idx_2\n",
    "#             missing_rec['index'] = 3\n",
    "#             arr = np.vstack((idx_1, idx_2, missing_rec))\n",
    "#             feat_arr.append(list(arr.flatten()))\n",
    "        \n",
    "    else:\n",
    "        feat_arr.append(list(np.array(records).flatten()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feats = []\n",
    "y_labels = []\n",
    "get_three_months(complete_prod_train, input_feats, y_labels, False)\n",
    "valid_dataset = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7235, 51)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(input_feats).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare testing data\n",
    "x_test = []\n",
    "testing_apis = []\n",
    "get_three_months(complete_prod_test, x_test, [], True)\n",
    "testing_apis = testing_apis[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 50), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = MLPRegressor(hidden_layer_sizes=(50,50), activation='relu', solver='adam', \n",
    "                         alpha=0.0001, batch_size='auto',\n",
    "                         learning_rate_init=0.001, power_t=0.5, max_iter=200, \n",
    "                         shuffle=True, random_state=None, tol=0.0001, verbose=False, \n",
    "                         warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                         early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n",
    "                         epsilon=1e-08)\n",
    "regressor.fit(input_feats, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "predictions = predictions[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(testing_apis)):\n",
    "    data.append([testing_apis[i], predictions[i]])\n",
    "output_df = pd.DataFrame(data, columns = ['Id', 'Predicted'])\n",
    "output_df.to_csv('three_yrs_cum.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3331"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
